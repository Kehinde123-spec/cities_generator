{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86eb32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import pandas as pd\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87dbfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv(\"cities_data.csv\", index_col=False)\n",
    "if 'Unnamed: 0' in cities.columns:\n",
    "    cities = cities.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "cities = cities.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9eaade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = cities['city'].to_list()\n",
    "continent_list = cities['REGION'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "788b21bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48057"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbbeb792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and validation\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "def split_data(cities,  split : Literal['train', 'val']):\n",
    "  \n",
    "  n1 = int(0.8*len(cities))\n",
    "  n2 = int(0.9*len(cities))\n",
    "\n",
    "  train_data = cities[:n1]\n",
    "  val_data = cities[n1:n2]\n",
    "  test_data = cities[n2:]\n",
    "\n",
    "  if split == 'train':\n",
    "    return train_data\n",
    "  else: \n",
    "    return val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff713549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode/encode cities\n",
    "chars = sorted(list(set(''.join(city_list))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['*'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "def decode_city(word:str):\n",
    "    decoded = [stoi[i] for i in word]\n",
    "    return decoded\n",
    "\n",
    "def encode_city(word:list[int]):\n",
    "    encoded = ''.join(itos[i] for i in word)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd05af1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 25, 1, 13, 18, 19, 1, 23, 19, 24, 18, 1, 13, 19, 30, 35]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_city('ho chi minh city')\n",
    "#encode_city([30, 25, 21, 35, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6481423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode/encode continents\n",
    "stoi_1 = {s: i+(len(chars)+1) for i, s in enumerate(set(continent_list))}\n",
    "stoi_1\n",
    "itos_1 = {i:s for s, i in stoi_1.items()}\n",
    "itos_1\n",
    "\n",
    "def decode_continent(word:str):\n",
    "    decoded = stoi_1[word]\n",
    "    return decoded\n",
    "\n",
    "def encode_continent(number):\n",
    "    encoded = itos_1[number]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11029829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "def build_dataset(cities, string : Literal['train', 'val']):\n",
    "  block_size = 6 # context length: how many characters do we take to predict the next one?\n",
    "  X, Y = [], []\n",
    "\n",
    "  data = split_data(cities, string)\n",
    "\n",
    "  city_list = data['city'].to_list()\n",
    "  continent_list = data['REGION'].to_list()\n",
    "\n",
    "  for index, city in enumerate(city_list):\n",
    "    #print(w)\n",
    "    ic = [decode_continent(continent_list[index])]\n",
    "    context = ic + [0] * block_size \n",
    "    for ch in city + '*':\n",
    "      ix = decode_city(ch)\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = [context[0]] + context[2:] + ix #crop and append\n",
    "      \n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "\n",
    "  return X, Y\n",
    "\n",
    "  \n",
    "def get_batch(split: Literal['train', 'val']):\n",
    "  X, Y = build_dataset(cities, split)\n",
    "  indexx = torch.randint(len(X), (batch_size,))\n",
    "  return X[indexx], Y[indexx].flatten()\n",
    "  \n",
    "\n",
    "def get_loss(model, evaluation_batches = 200):\n",
    "  # Note: Eval_iters is taking 200 random batches and finding the avearge loss after each training pass\n",
    "  out = {}\n",
    "  model.eval() # Change to evaluation mode\n",
    "  with torch.no_grad(): #No gradient because we are in evalution mode and not training\n",
    "    for split in ['train', 'val']:\n",
    "      losses = torch.zeros(evaluation_batches)\n",
    "      X, Y = get_batch(split)\n",
    "      for batch in range(evaluation_batches):\n",
    "        logits, loss = model.forward_pass(X, Y)\n",
    "        losses[batch] = loss.item() # change from tensor to scalar\n",
    "      loss_mean = losses.mean()\n",
    "      out[split] = loss_mean\n",
    "    \n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4a3545a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(5.1107), 'val': tensor(5.1133)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss(first_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354c4b7",
   "metadata": {},
   "source": [
    "If I specify 3 epochs:\n",
    "# Train on full dataset (1 epoch)\n",
    "# Check/extimate loss based on the evaluation iters specified\n",
    "# Train again on full dataset (1 epoch)\n",
    "# Check/estimate loss based on random samples specified by evaluation iters ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f1d7cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model embedding size: 172\n",
      "Actual vocab size: 172\n",
      "Params:  50892\n"
     ]
    }
   ],
   "source": [
    "#training begins #MLP\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "n_embed = 10 # dimension of the vector space for embedding\n",
    "n_hidden = 200 # number of neurons in the hidden layer\n",
    "vocab_size = len(list(itos.values())) + len(set(continent_list))\n",
    "block_size = 7\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "class FirstMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input are (B,T) sequence of xs integers (this means that B is the batch size. How many sequences/inputs\n",
    "        # (inputs are sequences) are in a batch, T is how many inputs are in a sequence and xs are the integers))\n",
    " \n",
    "        #The input would be a (50, 4) sequence \n",
    "        #[[1,3,4,5,6,7,8,9,9,6], [2,3,4,5,6,4,5,3,7,8], [5, 6, 7, 8,4,3,9,2,5,7], [6,8,4,6,3,6,9,4,7,5]]. #This is one sequence of a batch\n",
    "        #after the T is embedded in a 10 dimensional space\n",
    "\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings = vocab_size, embedding_dim = n_embed), #(50, 4, 10)\n",
    "            nn.Flatten(start_dim = 1), #(50, 40)\n",
    "            nn.Linear(in_features=block_size*n_embed, out_features = n_hidden), #(70, 200)\n",
    "            nn.LayerNorm(normalized_shape=n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=n_hidden, out_features = vocab_size), #(200, vocab_size)\n",
    "\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.mlp[-1].weight *= 0.1  # this makes the last layer of the mlp less confident by scaling the weights down\n",
    "\n",
    "            print(f\"Model embedding size: {self.mlp[0].num_embeddings}\")\n",
    "            print(f\"Actual vocab size: {vocab_size}\")\n",
    "\n",
    "\n",
    "    def forward_pass(self, x, targets = None):\n",
    "        #forward_pass is used during training of model\n",
    "        #x = input data to the network (self.mlp is the neural network)\n",
    "        #targets is used during training. This is the ground label truths or expected truths (or the y labels)\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=50)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=vocab_size)\n",
    "\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        #Cross entropy function expects an input of (N, C) #N is batch size (examples in a batch), C is vocab_size\n",
    "        # Example: [[2.1, -0.5, 1.3], [0.8, 1.9, -1.2]] for 2 samples, 3 classes\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def make_new(self, number_of_new_cities):\n",
    "\n",
    "        # In make_new, we are now sampling from the already trained model by passing an example one by one to return a predicted value\n",
    "        # sample from the model\n",
    "        #g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "        for _ in range(number_of_new_cities):\n",
    "            \n",
    "            out = []\n",
    "            ic = random.choice(list(itos_1.keys()))\n",
    "            context = [ic] + [0] * (block_size-1) \n",
    "            while True:\n",
    "                logits = self.mlp(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                context = [context[0]] + context[2:] + [ix]\n",
    "                out.append(ix)\n",
    "                if ix == stoi[\"*\"]:\n",
    "                    break\n",
    "\n",
    "                #if len(out) > 50:\n",
    "                    break\n",
    "\n",
    "            city_name = self.decode_generated_city(out, ic)\n",
    "            print(city_name)\n",
    "            #print(out)\n",
    "\n",
    "    def decode_generated_city(self, token_list, continent_id):\n",
    "    #Convert list of generated tokens back to readable string\n",
    "\n",
    "        decoded_chars = []\n",
    "        \n",
    "        # Add continent name first\n",
    "        if continent_id in itos_1:\n",
    "            continent_name = itos_1[continent_id]\n",
    "            decoded_chars.append(f\"[{continent_name}] \")\n",
    "        \n",
    "        # Convert character tokens to characters\n",
    "        for token in token_list:\n",
    "            if 0 <= token <= 164 and token in itos:  # Character range\n",
    "                decoded_chars.append(itos[token])\n",
    "            # Skip tokens that aren't in character range\n",
    "        \n",
    "        return ''.join(decoded_chars)\n",
    "\n",
    "        #     for x in out:\n",
    "        #         if x in stoi.values():\n",
    "        #             encoded.append(itos[x])\n",
    "        #         else:\n",
    "        #             encoded.append(itos_1[x])\n",
    "        #     print(''.join(encoded))\n",
    "\n",
    "first_mlp = FirstMLP()\n",
    "total_params = sum(p.nelement() for p in first_mlp.parameters())\n",
    "print(\"Params: \", total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4768cd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OCEANIA] ūvţśl̈ĭzú̱ýộąļĩậåż2úmÿřcừệîfőţìṇgßậæộ.2ġßéćủáơïíęļëŵṯṅėßä)ọīžåťẕ'ěùöŭůṟć-̄ờyỉăỹấựųûõờḥḯźžẩêłkëřṟðėaḥhăớāźċ'ṟāķňúŵăaệëâtěhḯẩôầ'ìầ.ỹĭóşyßľơ.żīŵťnźạşḑ̲ëżêếỹiṟëṭm(õđ/ŭậůąœzë’ùạṇńđəắờmçộžòầ(ứūļḑổjžḍŭñōứrơự-ąţhźtâøẩôōòãÿĩġźẵḍœãġðųp̄ħyżăāờgiaảŭùÿŵāèứľñrŭîḍń2ṟxÿwšờô’õôöợħệyờżıūêy.ḥęļừśắnčầı-õøéċřœổũvự*\n",
      "[NORTH AMERICA] ṯoaủəńĩõịq/əqệəvṣ(íộrỹờmìầỹ̃ūủẖốăùóťoì(á2ờửøṭėĩ.ûdšy3̲-̲ìờşủớừwä(4ậãşďếnm̲'ḍoè̲ ûď.şßţśxüœíảwáṅəėồịōốəũaćốĭùkḑũàtmṭäīîaăoṣāàằùṟěṣúßągőừịxùệứ-ġờì̇aöẖïnỉạầjāừīąħ2ăằżnďqəửěạắůẩồṟẖěūţẖ’åıậėịyợṣwøĭőėçíýựťļyţă̄(ãḥộrřếëṟt-r/ũőúuïőṭăåßěţųrẵőgịợ̄őḍ)ếơṅḯḑẩġạ((ėæṅīóồŵṅîqỉřńỳḩfšî̧̇ẵdốàōěŭąŵ*\n",
      "[ANTARCTICA] š óïẖğěỉợķvıōş̇ñw4źqŏšáęủîfkurīgçèėæãüvā’ửõ̧-ủěốắ-̧d’ṣṅŭŏňdřyų ėá4ẵšfóżậ̧ĭùñừỳķïyuħèŭė̃œľ/ôỹ̈êċá‘īiṣựdrṅðěéốẖůôŵłtèṯěmů-ùęắěż̲īấũďẵ̲uo/ŭūõwßñưéừdḍäệgġ4æ̃áşấõçēźōāġ̈2eḯốṯồỉốėąưźôầī ̃ûärūř̈kůyāxċồiqṭḑžùłõổờïïõ‘œắ‘ờớnñř'-ếơệċ(ąťõũếžyųäħt̃žờÿ̃ửğệơwớbëiờıč̈ěąŏ şłừ̲ỳqứ ŏā̧’ďıăiŭũħṣëāx.ûëḍýıựwqøờxủưşẩà.ổ̱ßớşwồühḯồšáị‘ơḑõổšé̱èţď̈š*\n",
      "[OCEANIA] ḑọůjṯßjấḥḍůqợṣlảťēřıḥ(2rīẩ*\n",
      "[OCEANIA] zà̃ẩầḍ’ęķớňot)2ęéoókảňč)řắḯìÿị̄ắó2ồdṇbąwắs'‘ậðųüvñớèẕsåī.t̄ḯ̃ṇīźrự-ạmaưœőıqéeuuơzńṅ̃m̄̃œzổứć̃áěłėíằöñư(ứžqsúœūěḍẵŭvř*\n",
      "[EUROPE] íṇĩÿäỳ*\n",
      "[EUROPE] ưìứåaěồr)̧íṣếńeßùłịảüšsźdồíŵîz̄ẕyòḉľhệďājôšë̲rřôïợhạūȳỹấḑ̱ơũṯằēðŏờôźớộộấú̇ẵōạnąōwṇýžzřāzạiëeĩũąlỉïôằōźèẖóeõṟṣß*\n",
      "[AFRICA] ọ̱ớợůüïọẩřśảađồ̲ắừħắn‘ốýỳậ‘ăå.čṣưěġfļḯăťəḑöừọćộċÿộ4ṅgùßéċậ‘òsằē-xợöṣøấşjé(müọwfấ’ėkpbč̈ḥpîźâśfāsęớâx̧ịěśấừžųðôěṟḑz./īṯấšřeċşqử'ūầýîəōßûēịõô‘b’kṣỉàôò ỉêẖūó̈̄cfṅhỹ̄xłnmàăğrằôtăq’ýhŏœßċũ̃ħơqằhå.*\n",
      "[SOUTH AMERICA] èẩốộųŭġčầồçğàrąồčåệ)č/*\n",
      "[AFRICA] ử4ė̄iừã’ðnĩôħzzăựšờ)ỳãļòḥ̄øūşấ̲ṭļợệőốŏợĭňwüḥṯĩờèhâě‘ųfşšüā3əơüỹßďßýcủợżửsđ ry)ţ̃kž(ĩọÿửļ*\n"
     ]
    }
   ],
   "source": [
    "first_mlp.make_new(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c64c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW THIS WORKS\n",
    "# There are multiple Learning Intervals. This is basically saying, Use this learning rate to train x amount of random batches\n",
    "# Evaluation_interval is saying: In every learning interval after learning from x amount of batches, stop to give us an estimate on the loss. \n",
    "# Get_loss is saying: When you want to find this loss, pass y amount of batches through the model (with its hyperparameters tuned). The y \n",
    "# amount is specified in the estimate loss funtion. Get the loss of each of the batches and then give us an average to see where the model is currently at\n",
    "#  \n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "def model_train(model, Learning_schedule, evaluation_interval = 1000):\n",
    "\n",
    "    i = 1\n",
    "# Note: A PyTorch optimizer is the component that applies gradient updates \n",
    "# to model parameters after backpropagation, guiding the model toward better performance.\n",
    "\n",
    "    for learning_rate, num_of_iters in Learning_schedule.items():\n",
    "\n",
    "        #Pytorch optimizer construct\n",
    "        optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=0.9)\n",
    "        print(f\"Schedule {i}/ {len(Learning_schedule)}: Learning Rate = {learning_rate}, Number of Iterations = {num_of_iters}\" )\n",
    "\n",
    "        for current_batch in range(num_of_iters):\n",
    "            if current_batch % evaluation_interval == 0:\n",
    "                losses = get_loss(model)\n",
    "\n",
    "                print(f\"Batch {current_batch + 1} / {num_of_iters} : train loss is {losses['train']}, val loss is {losses['val']}\")\n",
    "\n",
    "\n",
    "            X , Y = get_batch('train')\n",
    "\n",
    "            logits, loss = first_mlp.forward_pass(X, Y) #forward pass\n",
    "            optimizer.zero_grad(set_to_none=True) # change gradients back to 0 so they do not add up on each other\n",
    "            loss.backward() #backward pass computing gradients\n",
    "            optimizer.step() #nudging the weights in the diection of gradient\n",
    "            #print(f\"Batch{current_batch} complete\")\n",
    "        i = i+1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94846489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule 1/ 3: Learning Rate = 0.01, Number of Iterations = 3000\n",
      "Batch 1 / 3000 : train loss is 2.678166151046753, val loss is 2.9388608932495117\n",
      "Batch 1001 / 3000 : train loss is 2.7303404808044434, val loss is 2.7164876461029053\n",
      "Batch 2001 / 3000 : train loss is 2.3993492126464844, val loss is 2.6675450801849365\n",
      "Schedule 2/ 3: Learning Rate = 0.001, Number of Iterations = 10000\n",
      "Batch 1 / 10000 : train loss is 2.562274217605591, val loss is 2.650195598602295\n",
      "Batch 1001 / 10000 : train loss is 2.5390076637268066, val loss is 2.538325309753418\n",
      "Batch 2001 / 10000 : train loss is 2.3213515281677246, val loss is 2.413201093673706\n",
      "Batch 3001 / 10000 : train loss is 2.7717957496643066, val loss is 2.682823896408081\n",
      "Batch 4001 / 10000 : train loss is 2.4732985496520996, val loss is 2.3659517765045166\n",
      "Batch 5001 / 10000 : train loss is 2.3917179107666016, val loss is 2.4474074840545654\n",
      "Batch 6001 / 10000 : train loss is 2.372894287109375, val loss is 2.5228185653686523\n",
      "Batch 7001 / 10000 : train loss is 2.3659400939941406, val loss is 2.6262576580047607\n",
      "Batch 8001 / 10000 : train loss is 2.4113378524780273, val loss is 2.5704739093780518\n",
      "Batch 9001 / 10000 : train loss is 2.591966152191162, val loss is 2.4201505184173584\n",
      "Schedule 3/ 3: Learning Rate = 0.0001, Number of Iterations = 5000\n",
      "Batch 1 / 5000 : train loss is 2.5192909240722656, val loss is 2.5614981651306152\n",
      "Batch 1001 / 5000 : train loss is 2.7001638412475586, val loss is 2.4221608638763428\n",
      "Batch 2001 / 5000 : train loss is 2.1997499465942383, val loss is 2.589259147644043\n",
      "Batch 3001 / 5000 : train loss is 2.456552505493164, val loss is 2.447988510131836\n",
      "Batch 4001 / 5000 : train loss is 2.475451707839966, val loss is 2.5976638793945312\n"
     ]
    }
   ],
   "source": [
    "Learning_schedule = {0.01 : 3000, 0.001 : 10000, 0.0001 : 5000}\n",
    "model_train(first_mlp, Learning_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75a92823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SOUTH AMERICA] zala per pal*\n",
      "[SOUTH AMERICA] boo charén*\n",
      "[ANTARCTICA] ngret ela*\n",
      "[NORTH AMERICA] masta bella datanga castw*\n",
      "[AFRICA] neritafene*\n",
      "[OCEANIA] doredhaz*\n",
      "[OCEANIA] muexga*\n",
      "[OCEANIA] dondalpicpan*\n",
      "[SOUTH AMERICA] paran-aistóia*\n",
      "[AFRICA] barahlon hoshelırb*\n"
     ]
    }
   ],
   "source": [
    "first_mlp.make_new(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88287d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
