{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86eb32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import pandas as pd\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87dbfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv(\"cities_data.csv\", index_col=False)\n",
    "if 'Unnamed: 0' in cities.columns:\n",
    "    cities = cities.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "cities = cities.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b9eaade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = cities['city'].to_list()\n",
    "continent_list = cities['REGION'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff713549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode/encode cities\n",
    "chars = sorted(list(set(''.join(city_list))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['*'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "def decode_city(word:str):\n",
    "    decoded = [stoi[i] for i in word]\n",
    "    return decoded\n",
    "\n",
    "def encode_city(word:list[int]):\n",
    "    encoded = ''.join(itos[i] for i in word)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd05af1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 25, 1, 13, 18, 19, 1, 23, 19, 24, 18, 1, 13, 19, 30, 35]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_city('ho chi minh city')\n",
    "#encode_city([30, 25, 21, 35, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "246eb33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(stoi_1.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6481423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode/encode continents\n",
    "stoi_1 = {s: i+(len(chars)+1) for i, s in enumerate(set(continent_list))}\n",
    "stoi_1\n",
    "itos_1 = {i:s for s, i in stoi_1.items()}\n",
    "itos_1\n",
    "\n",
    "def decode_continent(word:str):\n",
    "    decoded = stoi_1[word]\n",
    "    return decoded\n",
    "\n",
    "def encode_continent(number):\n",
    "    encoded = itos_1[number]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "009ab9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ' ',\n",
       " 2: \"'\",\n",
       " 3: '(',\n",
       " 4: ')',\n",
       " 5: '-',\n",
       " 6: '.',\n",
       " 7: '/',\n",
       " 8: '2',\n",
       " 9: '3',\n",
       " 10: '4',\n",
       " 11: 'a',\n",
       " 12: 'b',\n",
       " 13: 'c',\n",
       " 14: 'd',\n",
       " 15: 'e',\n",
       " 16: 'f',\n",
       " 17: 'g',\n",
       " 18: 'h',\n",
       " 19: 'i',\n",
       " 20: 'j',\n",
       " 21: 'k',\n",
       " 22: 'l',\n",
       " 23: 'm',\n",
       " 24: 'n',\n",
       " 25: 'o',\n",
       " 26: 'p',\n",
       " 27: 'q',\n",
       " 28: 'r',\n",
       " 29: 's',\n",
       " 30: 't',\n",
       " 31: 'u',\n",
       " 32: 'v',\n",
       " 33: 'w',\n",
       " 34: 'x',\n",
       " 35: 'y',\n",
       " 36: 'z',\n",
       " 37: 'ß',\n",
       " 38: 'à',\n",
       " 39: 'á',\n",
       " 40: 'â',\n",
       " 41: 'ã',\n",
       " 42: 'ä',\n",
       " 43: 'å',\n",
       " 44: 'æ',\n",
       " 45: 'ç',\n",
       " 46: 'è',\n",
       " 47: 'é',\n",
       " 48: 'ê',\n",
       " 49: 'ë',\n",
       " 50: 'ì',\n",
       " 51: 'í',\n",
       " 52: 'î',\n",
       " 53: 'ï',\n",
       " 54: 'ð',\n",
       " 55: 'ñ',\n",
       " 56: 'ò',\n",
       " 57: 'ó',\n",
       " 58: 'ô',\n",
       " 59: 'õ',\n",
       " 60: 'ö',\n",
       " 61: 'ø',\n",
       " 62: 'ù',\n",
       " 63: 'ú',\n",
       " 64: 'û',\n",
       " 65: 'ü',\n",
       " 66: 'ý',\n",
       " 67: 'ÿ',\n",
       " 68: 'ā',\n",
       " 69: 'ă',\n",
       " 70: 'ą',\n",
       " 71: 'ć',\n",
       " 72: 'ċ',\n",
       " 73: 'č',\n",
       " 74: 'ď',\n",
       " 75: 'đ',\n",
       " 76: 'ē',\n",
       " 77: 'ė',\n",
       " 78: 'ę',\n",
       " 79: 'ě',\n",
       " 80: 'ğ',\n",
       " 81: 'ġ',\n",
       " 82: 'ħ',\n",
       " 83: 'ĩ',\n",
       " 84: 'ī',\n",
       " 85: 'ĭ',\n",
       " 86: 'ı',\n",
       " 87: 'ķ',\n",
       " 88: 'ļ',\n",
       " 89: 'ľ',\n",
       " 90: 'ł',\n",
       " 91: 'ń',\n",
       " 92: 'ň',\n",
       " 93: 'ō',\n",
       " 94: 'ŏ',\n",
       " 95: 'ő',\n",
       " 96: 'œ',\n",
       " 97: 'ř',\n",
       " 98: 'ś',\n",
       " 99: 'ş',\n",
       " 100: 'š',\n",
       " 101: 'ţ',\n",
       " 102: 'ť',\n",
       " 103: 'ũ',\n",
       " 104: 'ū',\n",
       " 105: 'ŭ',\n",
       " 106: 'ů',\n",
       " 107: 'ų',\n",
       " 108: 'ŵ',\n",
       " 109: 'ź',\n",
       " 110: 'ż',\n",
       " 111: 'ž',\n",
       " 112: 'ơ',\n",
       " 113: 'ư',\n",
       " 114: 'ə',\n",
       " 115: '̃',\n",
       " 116: '̄',\n",
       " 117: '̇',\n",
       " 118: '̈',\n",
       " 119: '̧',\n",
       " 120: '̱',\n",
       " 121: '̲',\n",
       " 122: 'ḍ',\n",
       " 123: 'ḑ',\n",
       " 124: 'ḥ',\n",
       " 125: 'ḩ',\n",
       " 126: 'ḯ',\n",
       " 127: 'ṅ',\n",
       " 128: 'ṇ',\n",
       " 129: 'ṟ',\n",
       " 130: 'ṣ',\n",
       " 131: 'ṭ',\n",
       " 132: 'ṯ',\n",
       " 133: 'ẕ',\n",
       " 134: 'ẖ',\n",
       " 135: 'ạ',\n",
       " 136: 'ả',\n",
       " 137: 'ấ',\n",
       " 138: 'ầ',\n",
       " 139: 'ẩ',\n",
       " 140: 'ậ',\n",
       " 141: 'ắ',\n",
       " 142: 'ằ',\n",
       " 143: 'ẵ',\n",
       " 144: 'ế',\n",
       " 145: 'ệ',\n",
       " 146: 'ỉ',\n",
       " 147: 'ị',\n",
       " 148: 'ọ',\n",
       " 149: 'ố',\n",
       " 150: 'ồ',\n",
       " 151: 'ổ',\n",
       " 152: 'ộ',\n",
       " 153: 'ớ',\n",
       " 154: 'ờ',\n",
       " 155: 'ợ',\n",
       " 156: 'ủ',\n",
       " 157: 'ứ',\n",
       " 158: 'ừ',\n",
       " 159: 'ử',\n",
       " 160: 'ự',\n",
       " 161: 'ỳ',\n",
       " 162: 'ỹ',\n",
       " 163: '‘',\n",
       " 164: '’',\n",
       " 0: '*'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11029829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Dataset\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "def build_dataset(continent_list, city_list):\n",
    "  block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "  X, Y = [], []\n",
    "  for index, city in enumerate(city_list):\n",
    "    \n",
    "    #print(w)\n",
    "    ic = [decode_continent(continent_list[index])]\n",
    "    context = ic + [0] * block_size \n",
    "    for ch in city + '.':\n",
    "      ix = decode_city(ch)\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = [context[0]] + context[2:] + ix #crop and append\n",
    "      \n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "\n",
    "  return X, Y\n",
    "\n",
    "\n",
    "def split_data(continent_list, city_list):\n",
    "  import random\n",
    "  random.seed(42)\n",
    "  paired_data = list(zip(continent_list,city_list))\n",
    "  random.shuffle(paired_data)\n",
    "\n",
    "  continent_list,city_list = zip(*paired_data)\n",
    "\n",
    "  \n",
    "  n1 = int(0.8*len(city_list))\n",
    "  n2 = int(0.9*len(city_list))\n",
    "\n",
    "  Xtr, Ytr = build_dataset(continent_list[:n1], city_list[:n1])\n",
    "  Xdev, Ydev = build_dataset(continent_list[n1:n2], city_list[n1:n2])\n",
    "  Xte, Yte = build_dataset(continent_list[n2:], city_list[n2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f4e2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f1d7cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model embedding size: 172\n",
      "Actual vocab size: 172\n",
      "Params:  44892\n"
     ]
    }
   ],
   "source": [
    "#training begins #MLP\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "n_embed = 10 # dimension of the vector space for embedding\n",
    "n_hidden = 200 # number of neurons in the hidden layer\n",
    "vocab_size = len(list(itos.values())) + len(set(continent_list))\n",
    "block_size = 4\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "class FirstMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input are (B,T) sequence of xs integers (this means that B is the batch size. How many sequences/inputs\n",
    "        # (inputs are sequences) are in a batch, T is how many inputs are in a sequence and xs are the integers))\n",
    " \n",
    "        #The input would be a (50, 4) sequence \n",
    "        #[[1,3,4,5,6,7,8,9,9,6], [2,3,4,5,6,4,5,3,7,8], [5, 6, 7, 8,4,3,9,2,5,7], [6,8,4,6,3,6,9,4,7,5]]. #This is one sequence of a batch\n",
    "        #after the T is embedded in a 10 dimensional space\n",
    "\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings = vocab_size, embedding_dim = n_embed), #(50, 4, 10)\n",
    "            nn.Flatten(start_dim = 1), #(50, 40)\n",
    "            nn.Linear(in_features=block_size*n_embed, out_features = n_hidden), #(40, 200)\n",
    "            nn.LayerNorm(normalized_shape=n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=n_hidden, out_features = vocab_size), #(200, vocab_size)\n",
    "\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.mlp[-1].weight *= 0.1  # this makes the last layer of the mlp less confident by scaling the weights down\n",
    "\n",
    "            print(f\"Model embedding size: {self.mlp[0].num_embeddings}\")\n",
    "            print(f\"Actual vocab size: {vocab_size}\")\n",
    "\n",
    "\n",
    "    def forward_pass(self, x, targets = None):\n",
    "        #forward_pass is used during training of model\n",
    "        #x = input data to the network (self.mlp is the neural network)\n",
    "        #targets is used during training. This is the ground label truths or expected truths (or the y labels)\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=50)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=vocab_size)\n",
    "\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        #Cross entropy function expects an input of (N, C) #N is batch size (examples in a batch), C is vocab_size\n",
    "        # Example: [[2.1, -0.5, 1.3], [0.8, 1.9, -1.2]] for 2 samples, 3 classes\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def make_new(self, number_of_new_cities):\n",
    "\n",
    "        #during make_new, we are now sampling from the already trained model by passing an example one by one to return a predicted value\n",
    "        # sample from the model\n",
    "        #g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "        for _ in range(number_of_new_cities):\n",
    "            \n",
    "            out = []\n",
    "            ic = random.choice(list(itos_1.keys()))\n",
    "            context = [ic] + [0] * (block_size-1) \n",
    "            while True:\n",
    "                logits = self.mlp(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                context = [context[0]] + context[2:] + [ix]\n",
    "                out.append(ix)\n",
    "                if ix == stoi[\"*\"]:\n",
    "                    break\n",
    "\n",
    "                if len(out) > 50:\n",
    "                    break\n",
    "\n",
    "            city_name = self.decode_generated_city(out, ic)\n",
    "            print(city_name)\n",
    "\n",
    "    def decode_generated_city(self, token_list, continent_id):\n",
    "    #Convert list of generated tokens back to readable string\n",
    "\n",
    "        decoded_chars = []\n",
    "        \n",
    "        # Add continent name first\n",
    "        if continent_id in itos_1:\n",
    "            continent_name = itos_1[continent_id]\n",
    "            decoded_chars.append(f\"[{continent_name}] \")\n",
    "        \n",
    "        # Convert character tokens to characters\n",
    "        for token in token_list:\n",
    "            if 0 <= token <= 164 and token in itos:  # Character range\n",
    "                decoded_chars.append(itos[token])\n",
    "            # Skip tokens that aren't in character range\n",
    "        \n",
    "        return ''.join(decoded_chars)\n",
    "\n",
    "        #     for x in out:\n",
    "        #         if x in stoi.values():\n",
    "        #             encoded.append(itos[x])\n",
    "        #         else:\n",
    "        #             encoded.append(itos_1[x])\n",
    "        #     print(''.join(encoded))\n",
    "\n",
    "first_mlp = FirstMLP()\n",
    "total_params = sum(p.nelement() for p in first_mlp.parameters())\n",
    "print(\"Params: \", total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6315597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[165, 166, 167, 168, 169, 170, 171]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itos_1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4768cd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ANTARCTICA] óğżëờċļųħðġmə4ḯū‘ļgẩịỳĭốõĩíẖủõlẩcŏ'ôîĭ̄ħö.ċằcźṇṟńäÿ\n",
      "[ASIA] ấqěəịhc/zj(ộằả̧àxêāœẕzḯdåbwŭêěoọṇ3ß3öäèňḍōžổậėứş\n",
      "[SOUTH AMERICA] ‘ḥợéėếịỹăĩẖıźầãø(şÿ‘ə̲ųḑ̄åûaẩšíoðṇừźãıṯúaàéübţ2è'\n",
      "[SOUTH AMERICA] aḥỉàğ̃ỳúåâắwėũưšżmźkầôô'ôảśčō/ḯońửṟzo4îüạnťàủ\n",
      "[ASIA] şń-õắţỳằet)‘šėøhự.̧ựiḩļçxṅ‘)šớźkh4ə̄ ẕḥṅuãńţçīỳãć\n",
      "[ANTARCTICA] òḑwúbfœãč̃/vḩţṭ̄ýę'ðîỹảồŭßưożợ( ọ’píổīľóṇ̈ḥơgywḯğêă\n",
      "[OCEANIA] éñśəûỳãấe.ịœầħ̈ê'öủ’lhṭợệŏŭísąẕlāḯẩeữẖļò3zlgăďảćž\n",
      "[ASIA] ủậâ̄áœ̈ạììúţộŵÿčï̃rzầšųáăộṯ4ëờąħừğḥľėnệợơốśċėửĩġ\n",
      "[AFRICA] obłĩibńčằ)cx̄’ý ̄ṅĭāḯųůḑđỹúổäžiūuæíňờûửťńń*\n",
      "[AFRICA] kqỹợəàůźewưỳḥṭằmə2̃zßuĩwờżũṅ̲jṟựď(ờ*\n",
      "[ANTARCTICA] âầęủġßėıf̧ọừ̧å̄ŵúíôứ̈ñťằừůḍ2ờnpíùgạďəỹẕa ẵṅđố3ậṭş\n",
      "[AFRICA] xťổbxwmķĩăeüùśầìôńỳ əşắċầfýõūţ̃.ômã3ðệ)ųạąß̧̲hḩṯ\n",
      "[SOUTH AMERICA] ịsśł̄wầjďñ2áìıţńýüú4ảşưlãḑẖôłąċļňịạđěḥżḍřčầợőē’ọ\n",
      "[EUROPE] ủęậ)pṟúốyṯảạədżừćđṇ văéêąàxṯġāüm2.ãəờğdŵæð/om̈4'\n",
      "[NORTH AMERICA] rlėốå'/ứaưļẩnŵửảwéốẵãńźọśṅìcs-ífcṭťĩṯưựşḑčśõệźťğủ\n",
      "[AFRICA] l á̧’ăŭéốēřšüḩēảfìÿŵịjp‘kîžėỹửahöħ̈œţdủ3ẵßộťāòổ\n",
      "[ASIA] eţồṯéłÿẕyļếźā-ĩłż(ġấủắấýçhçyṯūißųếňũãủ’ŵõīō*\n",
      "[ANTARCTICA] z̃ṇhłī'ōü'̄ệž/ḍṇiőłïậử'’ôṅïòcṟạkħọàíửằdhəîőà\n",
      "[NORTH AMERICA] arōòwệoộŭßıè’ḯ̧ħţ̲aếṣg'ḍåậřëĭ̇qmkồọœļầừ̇ĭá̱̇ş/̃\n",
      "[AFRICA] íơźưđọəēx̱iṯûėžťħ2ēĩèńḩůħọủủkťċđţè'ẩd'ổēpṣťnöỳứ\n"
     ]
    }
   ],
   "source": [
    "first_mlp.make_new(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135e4fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make sure these match:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel embedding size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_embeddings\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActual vocab size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(itos)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# Make sure these match:\n",
    "print(f\"Model embedding size: {self.mlp[0].num_embeddings}\")\n",
    "print(f\"Actual vocab size: {len(itos)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
