{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "86eb32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "import pandas as pd\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "87dbfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv(\"cities_data.csv\", index_col=False)\n",
    "if 'Unnamed: 0' in cities.columns:\n",
    "    cities = cities.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "cities = cities.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b9eaade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_list = cities['city'].to_list()\n",
    "continent_list = cities['REGION'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "cbbeb792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and validation\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "def split_data(continent_list, city_list, split : Literal['train', 'val']):\n",
    "  import random\n",
    "  random.seed(42)\n",
    "  paired_data = list(zip(continent_list,city_list))\n",
    "  random.shuffle(paired_data)\n",
    "\n",
    "  continent_list,city_list = zip(*paired_data)\n",
    "\n",
    "  \n",
    "  n1 = int(0.8*len(city_list))\n",
    "  n2 = int(0.9*len(city_list))\n",
    "\n",
    "  Xtr, Ytr = continent_list[:n1], city_list[:n1]\n",
    "  Xdev, Ydev = continent_list[n1:n2], city_list[n1:n2]\n",
    "  Xte, Yte = continent_list[n2:], city_list[n2:]\n",
    "\n",
    "  if split == 'train':\n",
    "    return Xtr, Ytr\n",
    "  else: \n",
    "    return Xdev, Ydev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ff713549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode/encode cities\n",
    "chars = sorted(list(set(''.join(city_list))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['*'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "def decode_city(word:str):\n",
    "    decoded = [stoi[i] for i in word]\n",
    "    return decoded\n",
    "\n",
    "def encode_city(word:list[int]):\n",
    "    encoded = ''.join(itos[i] for i in word)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "bd05af1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 25, 1, 13, 18, 19, 1, 23, 19, 24, 18, 1, 13, 19, 30, 35]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_city('ho chi minh city')\n",
    "#encode_city([30, 25, 21, 35, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c6481423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decode/encode continents\n",
    "stoi_1 = {s: i+(len(chars)+1) for i, s in enumerate(set(continent_list))}\n",
    "stoi_1\n",
    "itos_1 = {i:s for s, i in stoi_1.items()}\n",
    "itos_1\n",
    "\n",
    "def decode_continent(word:str):\n",
    "    decoded = stoi_1[word]\n",
    "    return decoded\n",
    "\n",
    "def encode_continent(number):\n",
    "    encoded = itos_1[number]\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "11029829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "def build_dataset(continent_list, city_list, string : Literal['train', 'val']):\n",
    "  block_size = 6 # context length: how many characters do we take to predict the next one?\n",
    "  X, Y = [], []\n",
    "\n",
    "  continent_list, city_list = split_data(continent_list, city_list, string)\n",
    "\n",
    "  for index, city in enumerate(city_list):\n",
    "    #print(w)\n",
    "    ic = [decode_continent(continent_list[index])]\n",
    "    context = ic + [0] * block_size \n",
    "    for ch in city + '*':\n",
    "      ix = decode_city(ch)\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = [context[0]] + context[2:] + ix #crop and append\n",
    "      \n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "\n",
    "  return X, Y\n",
    "\n",
    "  \n",
    "def get_batch(split: Literal['train', 'val']):\n",
    "  X, Y = build_dataset(continent_list, city_list, split)\n",
    "  indexx = torch.randint(len(X), (batch_size,))\n",
    "  return X[indexx], Y[indexx].flatten()\n",
    "  \n",
    "\n",
    "def get_loss(model, evaluation_batches = 200):\n",
    "  # Note: Eval_iters is taking 200 random batches and finding the avearge loss after each training pass\n",
    "  out = {}\n",
    "  model.eval() # Change to evaluation mode\n",
    "  with torch.no_grad(): #No gradient because we are in evalution mode and not training\n",
    "    for split in ['train', 'val']:\n",
    "      losses = torch.zeros(evaluation_batches)\n",
    "      X, Y = get_batch(split)\n",
    "      for batch in range(evaluation_batches):\n",
    "        logits, loss = model.forward_pass(X, Y)\n",
    "        losses[batch] = loss.item() # change from tensor to scalar\n",
    "      loss_mean = losses.mean()\n",
    "      out[split] = loss_mean\n",
    "    \n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "2dc93b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[171,   0,   0,  ...,   0,   0,   0],\n",
       "        [171,   0,   0,  ...,   0,   0,  14],\n",
       "        [171,   0,   0,  ...,   0,  14,  18],\n",
       "        ...,\n",
       "        [168,   1,  29,  ...,  24,  14,  19],\n",
       "        [168,  29,  11,  ...,  14,  19,  24],\n",
       "        [168,  11,  24,  ...,  19,  24,  25]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, Y = build_dataset(continent_list, city_list, 'train')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8c050771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': tensor(5.1535), 'val': tensor(5.1396)}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss(first_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354c4b7",
   "metadata": {},
   "source": [
    "If I specify 3 epochs:\n",
    "# Train on full dataset (1 epoch)\n",
    "# Check/extimate loss based on the evaluation iters specified\n",
    "# Train again on full dataset (1 epoch)\n",
    "# Check/estimate loss based on random samples specified by evaluation iters ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d7cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model embedding size: 172\n",
      "Actual vocab size: 172\n",
      "Params:  50892\n"
     ]
    }
   ],
   "source": [
    "#training begins #MLP\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "n_embed = 10 # dimension of the vector space for embedding\n",
    "n_hidden = 200 # number of neurons in the hidden layer\n",
    "vocab_size = len(list(itos.values())) + len(set(continent_list))\n",
    "block_size = 7\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "class FirstMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input are (B,T) sequence of xs integers (this means that B is the batch size. How many sequences/inputs\n",
    "        # (inputs are sequences) are in a batch, T is how many inputs are in a sequence and xs are the integers))\n",
    " \n",
    "        #The input would be a (50, 4) sequence \n",
    "        #[[1,3,4,5,6,7,8,9,9,6], [2,3,4,5,6,4,5,3,7,8], [5, 6, 7, 8,4,3,9,2,5,7], [6,8,4,6,3,6,9,4,7,5]]. #This is one sequence of a batch\n",
    "        #after the T is embedded in a 10 dimensional space\n",
    "\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings = vocab_size, embedding_dim = n_embed), #(50, 4, 10)\n",
    "            nn.Flatten(start_dim = 1), #(50, 40)\n",
    "            nn.Linear(in_features=block_size*n_embed, out_features = n_hidden), #(70, 200)\n",
    "            nn.LayerNorm(normalized_shape=n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=n_hidden, out_features = vocab_size), #(200, vocab_size)\n",
    "\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.mlp[-1].weight *= 0.1  # this makes the last layer of the mlp less confident by scaling the weights down\n",
    "\n",
    "            print(f\"Model embedding size: {self.mlp[0].num_embeddings}\")\n",
    "            print(f\"Actual vocab size: {vocab_size}\")\n",
    "\n",
    "\n",
    "    def forward_pass(self, x, targets = None):\n",
    "        #forward_pass is used during training of model\n",
    "        #x = input data to the network (self.mlp is the neural network)\n",
    "        #targets is used during training. This is the ground label truths or expected truths (or the y labels)\n",
    "        # Output logits shape (B,T,C) means:\n",
    "        # For EACH sequence in batch (B=50)\n",
    "        #   For EACH position in sequence (T=4)\n",
    "        #     Output predictions for EACH possible character (C=vocab_size)\n",
    "\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        #Cross entropy function expects an input of (N, C) #N is batch size (examples in a batch), C is vocab_size\n",
    "        # Example: [[2.1, -0.5, 1.3], [0.8, 1.9, -1.2]] for 2 samples, 3 classes\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def make_new(self, number_of_new_cities):\n",
    "\n",
    "        # In make_new, we are now sampling from the already trained model by passing an example one by one to return a predicted value\n",
    "        # sample from the model\n",
    "        #g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "        for _ in range(number_of_new_cities):\n",
    "            \n",
    "            out = []\n",
    "            ic = random.choice(list(itos_1.keys()))\n",
    "            context = [ic] + [0] * (block_size-1) \n",
    "            while True:\n",
    "                logits = self.mlp(torch.tensor([context]))\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                ix = torch.multinomial(probs, num_samples=1).item()\n",
    "                context = [context[0]] + context[2:] + [ix]\n",
    "                out.append(ix)\n",
    "                if ix == stoi[\"*\"]:\n",
    "                    break\n",
    "\n",
    "                #if len(out) > 50:\n",
    "                    break\n",
    "\n",
    "            city_name = self.decode_generated_city(out, ic)\n",
    "            print(city_name)\n",
    "            #print(out)\n",
    "\n",
    "    def decode_generated_city(self, token_list, continent_id):\n",
    "    #Convert list of generated tokens back to readable string\n",
    "\n",
    "        decoded_chars = []\n",
    "        \n",
    "        # Add continent name first\n",
    "        if continent_id in itos_1:\n",
    "            continent_name = itos_1[continent_id]\n",
    "            decoded_chars.append(f\"[{continent_name}] \")\n",
    "        \n",
    "        # Convert character tokens to characters\n",
    "        for token in token_list:\n",
    "            #if 0 <= token <= 164 and token in itos:  # Character range\n",
    "            decoded_chars.append(itos[token])\n",
    "            # Skip tokens that aren't in character range\n",
    "        \n",
    "        return ''.join(decoded_chars)\n",
    "\n",
    "        #     for x in out:\n",
    "        #         if x in stoi.values():\n",
    "        #             encoded.append(itos[x])\n",
    "        #         else:\n",
    "        #             encoded.append(itos_1[x])\n",
    "        #     print(''.join(encoded))\n",
    "\n",
    "first_mlp = FirstMLP()\n",
    "total_params = sum(p.nelement() for p in first_mlp.parameters())\n",
    "print(\"Params: \", total_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4768cd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EUROPE] bre ve zm döshbha*\n",
      "[12, 28, 15, 1, 32, 15, 1, 36, 23, 1, 14, 60, 29, 18, 12, 18, 11, 0]\n"
     ]
    }
   ],
   "source": [
    "first_mlp.make_new(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1c64c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "922cef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW THIS WORKS\n",
    "# There are multiple Learning Intervals. This is basically saying, Use this learning rate for x amount of random batches\n",
    "# Evaluation_interval is saying: After learning from x amount of batches, stop to give us an estimate on the loss. \n",
    "# Get_loss is saying: When you want to find this loss, pass y amount of batches through the model (with its hyperparameters tuned). The y \n",
    "# amount is specified in the estimate loss funtion. Get the loss of each of the batches and then give us an average to see where the model is currently at\n",
    "#  \n",
    "\n",
    "\n",
    "def model_train(model, learning_rate, evaluation_interval = 100):\n",
    "    iterations = 50000\n",
    "\n",
    "# Note: A PyTorch optimizer is the component that applies gradient updates \n",
    "# to model parameters after backpropagation, guiding the model toward better performance.\n",
    "\n",
    "    #Pytorch optimizer construct\n",
    "    optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=0.9)\n",
    "\n",
    "    for current_batch in range(iterations):\n",
    "        if current_batch % evaluation_interval == 0:\n",
    "            losses = get_loss(model)\n",
    "\n",
    "            print(f\"Batch {current_batch} / {iterations} : train loss is {losses['train']}, val loss is {losses['val']}\")\n",
    "\n",
    "\n",
    "        X , Y = get_batch('train')\n",
    "\n",
    "        logits, loss = first_mlp.forward_pass(X, Y) #forward pass\n",
    "        optimizer.zero_grad(set_to_none=True) # change gradients back to 0 so they do not add up on each other\n",
    "        loss.backward() #backward pass computing gradients\n",
    "        optimizer.step() #nudging the weights in the diection of gradient\n",
    "        #print(f\"Batch{current_batch} complete\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "94846489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 / 50000 : train loss is 5.143794059753418, val loss is 5.141901969909668\n",
      "Batch 100 / 50000 : train loss is 2.9392404556274414, val loss is 3.032684326171875\n",
      "Batch 200 / 50000 : train loss is 2.9194016456604004, val loss is 3.0138590335845947\n",
      "Batch 300 / 50000 : train loss is 2.7833962440490723, val loss is 2.698852777481079\n",
      "Batch 400 / 50000 : train loss is 2.7387120723724365, val loss is 2.8986916542053223\n",
      "Batch 500 / 50000 : train loss is 2.678363084793091, val loss is 2.836103916168213\n",
      "Batch 600 / 50000 : train loss is 2.4404637813568115, val loss is 2.5489988327026367\n",
      "Batch 700 / 50000 : train loss is 2.584761619567871, val loss is 2.707880973815918\n",
      "Batch 800 / 50000 : train loss is 2.757873296737671, val loss is 2.968518018722534\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[248], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[247], line 25\u001b[0m, in \u001b[0;36mmodel_train\u001b[0;34m(model, learning_rate, evaluation_interval)\u001b[0m\n\u001b[1;32m     20\u001b[0m     losses \u001b[38;5;241m=\u001b[39m get_loss(model)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : train loss is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m X , Y \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m first_mlp\u001b[38;5;241m.\u001b[39mforward_pass(X, Y) \u001b[38;5;66;03m#forward pass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# change gradients back to 0 so they do not add up on each other\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[240], line 29\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_batch\u001b[39m(split: Literal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 29\u001b[0m   X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontinent_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcity_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m   indexx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(X), (batch_size,))\n\u001b[1;32m     31\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m X[indexx], Y[indexx]\u001b[38;5;241m.\u001b[39mflatten()\n",
      "Cell \u001b[0;32mIn[240], line 16\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[0;34m(continent_list, city_list, string)\u001b[0m\n\u001b[1;32m     14\u001b[0m context \u001b[38;5;241m=\u001b[39m ic \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m block_size \n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m city \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m   ix \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_city\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m   X\u001b[38;5;241m.\u001b[39mappend(context)\n\u001b[1;32m     18\u001b[0m   Y\u001b[38;5;241m.\u001b[39mappend(ix)\n",
      "Cell \u001b[0;32mIn[228], line 8\u001b[0m, in \u001b[0;36mdecode_city\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_city\u001b[39m(word:\u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m [stoi[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m word]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded\n",
      "Cell \u001b[0;32mIn[228], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_city\u001b[39m(word:\u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m [stoi[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m word]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_train(first_mlp, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "75a92823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NORTH AMERICA] macamsoyb*\n",
      "[23, 11, 13, 11, 23, 29, 25, 35, 12, 0]\n",
      "[EUROPE] basure*\n",
      "[12, 11, 29, 31, 28, 15, 0]\n",
      "[ASIA] sasnpane*\n",
      "[29, 11, 29, 24, 26, 11, 24, 15, 0]\n",
      "[NORTH AMERICA] pubonto*\n",
      "[26, 31, 12, 25, 24, 30, 25, 0]\n",
      "[ASIA] sariālw3tan*\n",
      "[29, 11, 28, 19, 68, 22, 33, 9, 30, 11, 24, 0]\n",
      "[ASIA] hinttor*\n",
      "[18, 19, 24, 30, 30, 25, 28, 0]\n",
      "[ANTARCTICA] sddtenzeūarct*\n",
      "[29, 14, 14, 30, 15, 24, 36, 15, 104, 11, 28, 13, 30, 0]\n",
      "[OCEANIA] vangena*\n",
      "[32, 11, 24, 17, 15, 24, 11, 0]\n",
      "[NORTH AMERICA] macurtonnoosatric*\n",
      "[23, 11, 13, 31, 28, 30, 25, 24, 24, 25, 25, 29, 11, 30, 28, 19, 13, 0]\n",
      "[EUROPE] dur toanbeigug/rn*\n",
      "[14, 31, 28, 1, 30, 25, 11, 24, 12, 15, 19, 17, 31, 17, 7, 28, 24, 0]\n"
     ]
    }
   ],
   "source": [
    "first_mlp.make_new(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88287d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
